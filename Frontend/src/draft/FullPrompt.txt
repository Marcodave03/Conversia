import React, { useState, useEffect } from "react";
import { Canvas } from "@react-three/fiber";
import { Experience } from "./components/Experience";
import Header from "./components/Header";
import bgImage from "./assets/conversia-bg.png";
import { MouthCue } from "./components/Avatar";
import { motion, AnimatePresence } from 'framer-motion';


type Message = {
  message: string;
  sender: string;
  direction: "incoming" | "outgoing";
};

type InterviewProps = {
  interview_prompt: string | undefined;
};

const App: React.FC<InterviewProps> = () => {
  const [currentExpression, setCurrentExpression] = useState<string | null>(
    null
  );
  const [modelUrl, setModelUrl] = useState<string>("/models/girl1.glb"); // default avatar
  const [backgroundUrl, setBackgroundUrl] = useState<string>(bgImage); // use default bg as fallback

  // Hide intro after animation finishes
  useEffect(() => {
    const timer = setTimeout(() => {
      setShowIntro(false);
    }, 3500); // typing duration + slide

    return () => clearTimeout(timer);
  }, []);

  const [currentAnimation, setCurrentAnimation] = useState<string | null>(null);
  const [currentMouthCues, setCurrentMouthCues] = useState<MouthCue[]>([]);
  const [audioDuration, setAudioDuration] = useState<number>(0);
  const [messages, setMessages] = useState<Message[]>([]);
  const [isTyping, setIsTyping] = useState(false);
  const [userInput, setUserInput] = useState("");
  const [typingText, setTypingText] = useState("");
  const [isSpeechEnabled, setIsSpeechEnabled] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(
    null
  );
  const [loadingTranscription, setLoadingTranscription] = useState(false);
  const [showIntro, setShowIntro] = useState(true);

  const handleSend = async () => {
    if (!userInput.trim()) return;

    const newMessage: Message = {
      message: userInput,
      direction: "outgoing",
      sender: "Aku",
    };

    const newMessages = [...messages, newMessage];

    setMessages(newMessages);
    setUserInput("");
    setIsTyping(true);
    await processMessageToChatGPT(newMessages);
  };

  async function processMessageToChatGPT(chatMessages: Message[]) {
    try {
      const lastMessage = chatMessages[chatMessages.length - 1];

      const response = await fetch("http://localhost:5555/chat", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ message: lastMessage.message }),
      });

      const data = await response.json();
      const fullText = data.message?.text || "Maya belum bicara ya...";
      const facialExpression = data.message?.facialExpression || null;
      const animation = data.message?.animation || null;
      const mouthCues = data.message?.lipsync?.mouthCues || [];
      const soundDuration = data.message?.lipsync?.metadata?.duration || 2;
      const audioUrl = "http://localhost:5555/audios/response.mp3";

      setTypingText(""); // Clear any previous typing
      setCurrentExpression(facialExpression);
      setCurrentAnimation(animation);
      setCurrentMouthCues(mouthCues);
      setAudioDuration(soundDuration * 1000);
      setIsTyping(true);

      // Initialize audio
      let audio: HTMLAudioElement | null = null;
      let audioDuration = 0;

      if (isSpeechEnabled && audioUrl) {
        try {
          // Bypass browser cache by appending a timestamp
          const freshAudioUrl = `${audioUrl}?t=${new Date().getTime()}`;
          const audioResponse = await fetch(freshAudioUrl);
          const audioBlob = await audioResponse.blob();
          const audioObjectUrl = URL.createObjectURL(audioBlob);

          audio = new Audio(audioObjectUrl);

          // add audio event listeners
          audio.onplay = () => setIsSpeaking(true);
          audio.onended = () => setIsSpeaking(false);

          await audio.play();

          audioDuration = audio.duration * 1000 || 2000; // Use duration to calculate typing delay
        } catch (err) {
          console.error("Error fetching or playing audio:", err);
        }
      }

      // Determine typing speed based on audio duration
      const duration = audioDuration || fullText.length * 50; // fallback
      const interval = duration / fullText.length;

      let index = 0;
      let lastTime = performance.now();

      const typeChar = (time: number) => {
        if (time - lastTime >= interval && index < fullText.length) {
          setTypingText((prev) => prev + fullText.charAt(index));
          index++;
          lastTime = time;
        }

        if (index < fullText.length) {
          requestAnimationFrame(typeChar);
        } else {
          // Typing complete
          setMessages((prev) => [
            ...prev,
            {
              message: fullText,
              sender: "Maya",
              direction: "incoming",
            },
          ]);
          setTypingText("");
          setIsTyping(false);
        }
      };

      requestAnimationFrame(typeChar);
    } catch (error) {
      console.error("Error talking to backend:", error);
      setIsTyping(false);
    }
  }

  const toggleSpeech = () => {
    setIsSpeechEnabled(!isSpeechEnabled);
  };

  const toggleRecording = async () => {
    if (isRecording) {
      mediaRecorder?.stop();
      setIsRecording(false);
    } else {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        const recorder = new MediaRecorder(stream);
        const audioChunks: Blob[] = [];

        recorder.ondataavailable = (event) => {
          audioChunks.push(event.data);
        };

        recorder.onstop = async () => {
          const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
          const formData = new FormData();
          formData.append("audio", audioBlob, "recording.webm");

          try {
            setLoadingTranscription(true); // === ADD THIS ===
            const response = await fetch(
              "http://localhost:5555/speech-to-text/full",
              {
                method: "POST",
                body: formData,
              }
            );

            const data = await response.json();
            if (data?.message?.text) {
              const newMessage: Message = {
                message: data.transcription.trim(),
                direction: "outgoing",
                sender: "Aku",
              };
              setMessages((prev) => [...prev, newMessage]);
              setIsTyping(true);
              await processMessageToChatGPT([...messages, newMessage]);
            }
          } catch (err) {
            console.error("Speech-to-Text failed:", err);
          } finally {
            setLoadingTranscription(false); // === ADD THIS ===
          }
        };

        recorder.start();
        setMediaRecorder(recorder);
        setIsRecording(true);
      } catch (err) {
        console.error("Failed to start recording:", err);
      }
    }
  };

  return (
    <>
    <AnimatePresence>
        {showIntro && (
          <motion.div
            className="fixed inset-0 bg-black flex justify-center items-center z-[1001]"
            initial={{ y: 0 }}
            animate={{ y: 0 }}
            exit={{ y: "-100%" }}
            transition={{ duration: 1 }}
          >
            <h1 className="text-blue-500 text-5xl md:text-7xl font-bold typing-effect">
              Conversia
            </h1>
          </motion.div>
        )}
      </AnimatePresence>

    <div
      className="h-screen w-full flex flex-col overflow-hidden"
      style={{
        backgroundImage: `url(${backgroundUrl})`,
        backgroundSize: "cover",
        backgroundPosition: "center",
        transition: "background-image 0.5s ease",
      }}
    >
     
     <Header setModelUrl={setModelUrl} setBackgroundUrl={setBackgroundUrl} />

      <div className="flex-1 flex">
        <div className="w-full h-full relative">
          {/* Chat messages */}
          <div
            className="h-[70vh] max-h-[70vh] overflow-y-auto space-y-4 p-4 absolute top-[10%] left-[55%] w-[43%] z-30"
            style={{
              position: "relative",
            }}
          >
            <div className="fixed top-0 left-0 w-full h-32 bg-gradient-to-b from-[#000000]/30 to-transparent z-40 pointer-events-none"></div>

            {messages.map((msg, index) => (
              <div
                key={index}
                className={`flex ${
                  msg.direction === "outgoing" ? "justify-end" : "justify-start"
                }`}
              >
                <div
                  className={`max-w-[60%] p-3 rounded-xl text-lg ${
                    msg.sender === "Maya"
                      ? "bg-white"
                      : "bg-blue-500 text-white"
                  }`}
                >
                  {msg.message}
                </div>
              </div>
            ))}

            {isTyping && typingText && (
              <div className="flex justify-start">
                <div className="bg-white p-3 rounded-xl text-lg">
                  {typingText}
                </div>
              </div>
            )}
            {loadingTranscription && (
              <div className="flex justify-center py-4">
                <div className="w-8 h-8 border-4 border-blue-500 border-dashed rounded-full animate-spin"></div>
              </div>
            )}
          </div>

          {/* Avatar (with lower z-index) */}
          <div className="absolute left-0 bottom-0 w-[50vw] h-[100vh] z-20 bg-transparent">
            <Canvas
              shadows
              camera={{ position: [0, -0.5, 1], fov: 10 }}
              style={{ width: "100%", height: "100%" }}
              gl={{ alpha: true, preserveDrawingBuffer: true }}
            >
              <Experience
                expression={currentExpression}
                animation={currentAnimation}
                mouthCues={currentMouthCues}
                audioDuration={audioDuration}
                modelUrl={modelUrl}
              />
            </Canvas>
          </div>
        </div>
      </div>

      {/* Input Section */}
      <div className="absolute z-[10] bottom-8 right-10 chats input-container bg-gray-800 bg-opacity-90 h-[7vh] flex items-center w-[43%] mx-auto rounded-full px-4">
        <input
          type="text"
          value={userInput}
          onChange={(e) => setUserInput(e.target.value)}
          placeholder="Start typing ..."
          className="border-none bg-transparent w-full text-white placeholder-white placeholder-opacity-70 text-2xl focus:outline-none px-4 py-2"
          onKeyPress={(e) => {
            if (e.key === "Enter") {
              handleSend();
            }
          }}
          style={{
            paddingLeft: "50px",
            paddingTop: "4px",
          }}
        />

        {/* Speech section */}
        <div className="flex gap-4 items-center">
          {/* Recording button */}
          <span
            className={`text-white text-4xl cursor-pointer transition-opacity duration-300 ${
              isRecording ? "text-red-500 animate-pulse" : "opacity-50"
            }`}
            onClick={toggleRecording}
          >
            🎙️
          </span>

          {/* Text-to-Speech toggle button */}
          <span
            className={`text-white text-4xl cursor-pointer transition-opacity duration-300 ${
              isSpeechEnabled ? "opacity-100" : "opacity-50"
            } ${isSpeaking ? "animate-pulse" : ""}`}
            onClick={toggleSpeech}
          >
            🔈
          </span>

          {/* Listening status text */}
          {isRecording && (
            <div className="flex items-center ml-2 animate-pulse">
              <span className="text-red-500 text-2xl">🎤</span>
              <span className="text-red-500 font-semibold ml-2">
                Recording...
              </span>
            </div>
          )}
        </div>
      </div>
    </div>
    </>
  );
};

export default App;,,,,,,

import React, { useState, useEffect } from "react";
import { FaUser, FaBook, FaImages } from "react-icons/fa";
import About from "./Background";
import Profile from "./Profile";
import AvatarPick from "./AvatarPick";
import logo from "../assets/conversia-lg.png";
import { useWallet } from "@suiet/wallet-kit";
import { useNavigate } from "react-router-dom";

type HeaderProps = {
  setModelUrl: (url: string) => void;
  setBackgroundUrl: (url: string) => void; 
};


const Header: React.FC<HeaderProps> = ({ setModelUrl,setBackgroundUrl }) => {
  const [showProfile, setShowProfile] = useState(false);
  const [showBackground, setShowBackground] = useState(false);
  const [showAvatar, setShowAvatar] = useState(false);
  const wallet = useWallet();
  const navigate = useNavigate();

  // Redirect to /landing if wallet is disconnected
  useEffect(() => {
    if (wallet.status === "disconnected") {
      navigate("/landing");
    }
  }, [wallet.status, navigate]);

  // Debug: show selected model URL (you can remove this in production)
  // useEffect(() => {
  //   if (modelUrl) {
  //     console.log("Selected avatar model:", modelUrl);
  //   }
  // }, [modelUrl]);

  return (
    <>
      {/* Conversia Logo */}
      <a
        href="/"
        className="fixed top-6 left-4 z-50 bg-white/10 backdrop-blur-xl rounded-full shadow-md p-2 transition hover:opacity-90"
      >
        <img
          src={logo}
          alt="Conversia Logo"
          className="h-12 w-auto object-contain"
        />
      </a>

      {/* Navigation Bar */}
      <header className="fixed top-6 left-1/2 transform -translate-x-1/2 z-50 bg-white bg-opacity-10 backdrop-blur-xl rounded-full shadow-lg px-8 py-4">
        <nav className="flex gap-8 text-white text-xl font-semibold items-center">
          <button
            onClick={() => setShowProfile(true)}
            className="flex items-center gap-2 hover:opacity-80"
          >
            <FaBook className="text-teal-400 text-2xl" />
            Profile
          </button>

          <button
            onClick={() => setShowBackground(true)}
            className="flex items-center gap-2 hover:opacity-80"
          >
            <FaImages className="text-blue-400 text-2xl" />
            Background
          </button>

          <button
            onClick={() => setShowAvatar(true)}
            className="flex items-center gap-2 hover:opacity-80"
          >
            <FaUser className="text-yellow-400 text-2xl" />
            Avatar
          </button>
        </nav>
      </header>

      {/* Disconnect Button */}
      {wallet.status === "connected" && (
        <button
          onClick={wallet.disconnect}
          className="fixed top-6 right-4 z-50 bg-red-600 hover:bg-red-700 text-white font-medium px-6 py-3 rounded-full shadow-lg transition"
        >
          Disconnect
        </button>
      )}

      {/* Popups */}
      {showProfile && <Profile onClose={() => setShowProfile(false)} />}
      {showBackground && (
        <About
          onClose={() => setShowBackground(false)}
          onSelectBackground={(url) => setBackgroundUrl(url)} // ✅ pass back
        />
      )}
      {showAvatar && (
        <AvatarPick
          userId={2}
          onClose={() => setShowAvatar(false)}
          onSelectAvatar={(modelUrl) => setModelUrl(modelUrl)}
        />
      )}
    </>
  );
};

export default Header;



import { CameraControls, ContactShadows, Environment } from "@react-three/drei";
import { useEffect, useRef } from "react";
import { Avatar } from "./Avatar";

// Import the MouthCue type from Avatar
import type { MouthCue } from "./Avatar";

type ExperienceProps = {
  expression: string | null;
  animation: string | null;
  mouthCues: MouthCue[];
  audioDuration: number;
  modelUrl: string;
};

export const Experience: React.FC<ExperienceProps> = ({
  expression,
  animation,
  mouthCues,
  audioDuration,
  modelUrl,
}) => {
  const cameraControls = useRef<CameraControls>(null);

  useEffect(() => {
    cameraControls.current?.setLookAt(0, 2, 5, 0, 1.5, 0);
  }, [modelUrl]);

  return (
    <>
      <CameraControls ref={cameraControls} />
      <Environment preset="sunset" />
      <Avatar
        key={modelUrl}
        expression={expression}
        animation={animation}
        mouthCues={mouthCues}
        audioDuration={audioDuration}
        modelUrl={modelUrl}
      />
      <ContactShadows opacity={0.7} />
    </>
  );
};







import { CameraControls, ContactShadows, Environment } from "@react-three/drei";
import { useEffect, useRef } from "react";
import { Avatar } from "./Avatar";

// Import the MouthCue type from Avatar
import type { MouthCue } from "./Avatar";

type ExperienceProps = {
  expression: string | null;
  animation: string | null;
  mouthCues: MouthCue[];
  audioDuration: number;
  modelUrl: string;
};

export const Experience: React.FC<ExperienceProps> = ({
  expression,
  animation,
  mouthCues,
  audioDuration,
  modelUrl,
}) => {
  const cameraControls = useRef<CameraControls>(null);

  useEffect(() => {
    cameraControls.current?.setLookAt(0, 2, 5, 0, 1.5, 0);
  }, [modelUrl]);

  return (
    <>
      <CameraControls ref={cameraControls} />
      <Environment preset="sunset" />
      <Avatar
        key={modelUrl}
        expression={expression}
        animation={animation}
        mouthCues={mouthCues}
        audioDuration={audioDuration}
        modelUrl={modelUrl}
      />
      <ContactShadows opacity={0.7} />
    </>
  );
};



from backend 



router.get("/chat-history/:user_id/:model_id", ChatHistoryController.getHistory);
router.post("/chat-history/:user_id/:model_id", ChatHistoryController.addMessage);

import ChatHistory from "../models/ChatHistory.js";
import User from "../models/User.js";

const ChatHistoryController = {
  async getHistory(req, res) {
    const { user_id, model_id } = req.params;
    const history = await ChatHistory.findAll({
      where: { user_id, model_id },
      order: [["createdAt", "ASC"]],
    });
    res.json(history);
  },

  async addMessage(req, res) {
    const { user_id, model_id } = req.params;
    const { message, sender } = req.body;

    const chat = await ChatHistory.create({ user_id, model_id, message, sender });
    res.status(201).json(chat);
  },
};

export default ChatHistoryController;




i want chathistory at my frontend stored all in database, i want chathistory based on user_id where i assume in web3 to get user_id its use sui wallet other than sui id its endpoint also need model_id ( avatar model ) for context im making avatar where avatar has each chat , eaach user has each avatar with each chat. now if i change the avatar i want chat history also change based on the avatar